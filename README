# Distributed Token Management System

## Contributors
- Shalev Azulay 206996621
- Aviel Raclaw 318965647

## [Implementation](https://github.com/Barvaziyel/Distributed-System-Omission-Faults)

### Implementation Overview

#### Initial Setup and Token Management

##### Token Objects:
Each token is characterized by:
- **ID:** A unique identifier.
- **Version:** Starts at 1 and increments with each transaction.
- **Owner ID:** Ranges from 1 to 6, corresponding to the six clients.

##### Clients:
Six clients are initialized, each owning ten tokens. The servers' initial data structures reflect this token distribution, ensuring all servers start with a consistent view of token ownership.

#### Client Actions
- **gettokens:** Clients can request details on all tokens owned by any client. This action is typically used to verify ownership before initiating transactions.
- **pay:** Enables a client to transfer a token they own to another client. This action updates the token’s ownership and increments its version. The initiating client receives an "OK" response, confirming the transaction.

#### Dynamic Server-Client Role Transformation
- **Server to Client Transformation:** Allows a server to become a client if there are more than three servers.
- **Client to Server Transformation:** Permits a client to become a server if there are fewer than seven servers. This process involves synchronization to update the new server with the latest system state.

### Safety and Liveness Testing

#### Manual Request Testing
Enables interactive testing through manual requests such as getTokens, pay, and role transformations. This mode also allows for querying the state of the network or individual nodes.

#### Liveness Tests
Ensures that every non-faulty client receives a response to their requests, confirming the system's ability to avoid deadlocks or indefinite hangs even under fault conditions. By not “getting stuck”, the liveness is proved.

#### Safety Tests
- **Initial Run with Faults and Delays:** Tests the system's resilience under simulated network delays and server faults, ensuring robustness against disruptions.
- **Re-Run in a Fault-Free Environment:** Validates that the system maintains consistent and correct final states, independent of earlier disruptions.

### Concurrent Random Automatic Request Testing
This testing phase introduces automated, randomized client actions (like getTokens and pay), executed at random intervals of up to two seconds. This simulates an environment with unsynchronized and unpredictable client requests.

After the automated tests, the network is shut down, and the actions are re-executed in a controlled, fault-free environment to ensure that the system's core functions are deterministic and reliable.

### Highlights
- **Concurrency and Synchronization:** Utilizes threading to handle client requests asynchronously introduces real-world conditions into the simulation, including potential delays and the handling of simultaneous requests.
- **Data Integrity and System Stability:** Extensive use of locks and thread-safe queues ensures that even during dynamic role transformations and concurrent access scenarios, data integrity and system stability are maintained.
- **Adaptability and Fault Tolerance:** The architecture supports seamless transitions between server and client roles without compromising the system’s ability to handle up to ![n/2](https://render.githubusercontent.com/render/math?math=%5Cfrac{n}{2}) omission faults, where `n` is the number of servers.
